\section{Numerical methods}

\subsection{Numerical Jacobian computation}
\label{sec:numerical-jacobians}
The Jacobian $J_x$ is the matrix of all first-order partial derivatives of a vector-valued function.
For the prediction and measurement model, the Jacobians of  the functions $f$ and $h$ \footnote{The equations are shown for an ``$f$'', but are of course the same for $f$ and $h$} are needed with respect to the state $x$ (where $n = \mathrm{length}(x)$).
The Jacobian of any function with respect to $x$ is defined as% \cite{martins2021}
\begin{align}
    J_x = \frac{\partial f}{\partial x}
    = \begin{bmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
    \end{bmatrix}
\end{align}
Jacobians may be derived analytically from the function, but this is often very cumbersome. % and prone to errors.
Therefore, it is common practice to compute the matrix numerically.
A frequently used method is using \textit{finite differences}, however this is known to be not very efficient nor accurate, and has issues with numerical instability \cite{martins2021}.

\subsubsection{Complex-step differentiation}
A more efficient way to calculate Jacobian matrices numerically is to use \textit{complex step differentiation}.
In this method with step size $\varepsilon$, the $k$-th column of the matrix ($J_{x_k}$) is \cite{martins2003}
\begin{align}
    J_{x_k} &= \mathrm{Im} \left( \frac{f(t, x + \mathrm{i} \varepsilon e_k, u)}{\varepsilon} \right)
    \label{eq:appendix_jacobian_complex_step}
\end{align}
where $e_k$ is the $k$-th unit vector in the state-space (i.e. $e \in X$, and $e_1 = [\begin{smallmatrix} 1 & 0 & \cdots & 0 \end{smallmatrix}]^T$, $e_2 = [\begin{smallmatrix} 0 & 1 & \cdots & 0 \end{smallmatrix}]^T$, etc.).
This must be computed for each entry of $x$, so that the Jacobian is 
\begin{align}
    J_x &= \begin{bmatrix}
        J_{x_1} & J_{x_2} & \cdots & J_{x_n} 
    \end{bmatrix} 
\end{align}
The evaluation of the functions with a complex state vector $[\begin{smallmatrix} x_1 + \mathrm{i} & x_2 & \cdots & x_n \end{smallmatrix}]^T$ seems very unintuitive, but works for every holomorphic $\mathbb{R}^n \rightarrow \mathbb{R}^m$ function\footnote{for implementation this requires that the function can be evaluated with complex inputs} \cite{martins2021}.
Complex-step differentiation also is numerically stable, and has also an $\mathrm{O}(h^2)$ error size \cite{martins2003}. 
Both finite differences and complex-step scale linearly with the number of variables, but the finite difference method requires two function evaluations per variable, while complex-step requires only one.


\subsection{Numerical Integration methods}
\label{sec:numerical-integration}
To get a new discrete state for the prediction process of the EKF from the continuous-time model, the nonlinear differential equations 
\begin{align}
    \dot x(t) &= f(t, x, u) & x(0) &= x_{n} 
\end{align}
have to be solved. 
As the last state estimate is available as an initial value, this can be expressed as an \textit{Initial Value Problem} (IVP). 
The IVP can be solved by numerical integration, see Equation \ref{eq:estimator-ivp}
\begin{align}
     \hat x_k^- &= \underbrace{\int_{t-dt}^{t} \dot{\hat x}(t,x,u) d\tau}_{f_D(x,u)}
     &
     \hat x (t-dt) &= \hat x_{k-1}^+
\end{align}
To minimize computational effort the integration methods should be explicit, as the filter in the prediction step has only past state information. 
An implicit method would require (potentially unstable) root finding methods, which are computationally expensive\footnote{Inverting the Jacobian in Newtons method}.

\subsubsection{Explicit Runge-Kutta method}
The explicit $s$-stage \textit{Runge-Kutta} method is compactly written as \cite{griffiths2010}
\begin{align}
    x_{n+1} &= x_n + \varepsilon \sum_{i=1}^{s} c_i k_i
    \\
    k_i &= f \left( t_n + a_i \varepsilon \, , \; x_n + \varepsilon \sum_{j=1}^{i-1} b_{ij} k_j \, , \ u \right) 
\end{align}
with the time step size $\varepsilon$, which in case of the EKF is the sampling time (i.e. the time since the last filter iteration).
The constants $a_i$, $b_{ij}$, and $c_i$ are the entries of the vectors $a$ and $c$ and the matrix $B$, displayed in a \textit{Butcher tableau}
\begin{equation}
    \begin{array}{c|c}
        a & B \\
        \hline 
        & c^T
        \end{array}
    \label{eq:appendix_general_butcher}
\end{equation}
These can be very large, as in the \textit{Dormand-Prince} method (RK45, which is the standard solver in Matlab).
For our cases we will keep the tableaus small.
The simplest, first order ($s=1$) method is explicit Euler, with the butcher tableau \cite{griffiths2010}
\begin{equation}
    \begin{array}{c|cc}
        0 & 0 \;\\
        \hline 
        & 1 \;
        \end{array}
    \label{eq:appendix_euler_butcher}
\end{equation}
which results in the simple formula
\begin{align}
    x_{n+1} &= x_n + \varepsilon f \left( t_n, \; x_n, \ u_n \right)
\end{align}
Other integration methods can be formulated similarly, for example the discrete update function of the quaternion kinematics\footnote{As unit quaternions are used for attitude description, they have to be either normed after each Euler integration, or the discrete update has to follow their geometric structure, as shown in the Equation above.}. % in Equation \ref{eq:quaternion-update}.
The second order ($s=2$) butcher tableau of the Heun method \cite{griffiths2010} is 
% which is used for the prediction covariance, is  
\begin{equation}
    \begin{array}{c|cc}
        0 &  &   \\
        1 & 1 &  \\
        \hline 
        & \frac{1}{2} & \frac{1}{2} 
        \end{array}
    \label{eq:appendix_heun_butcher}
\end{equation} 
The fourth order ($s=4$) butcher tableau of the classical Runge-Kutta method RK4 \cite{griffiths2010} is 
%which is used for solving the state prediction, is 
\begin{equation}
    \begin{array}{c|cccc}
        0 &  &  &  &  \\
        \frac{1}{2} & \frac{1}{2} &  &  &  \\
        \frac{1}{2} & 0 & \frac{1}{2} & & \\
        1 & 0 & 0 & 1 &  \\
        \hline 
        & \frac{1}{6} & \frac{1}{3} & \frac{1}{3} & \frac{1}{6}
    \end{array}
    \label{eq:appendix_rk4_butcher}
\end{equation}